{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oksanaerm/gesture_recognition/myenv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import zipfile\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory:  /Users/oksanaerm/gesture_recognition\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "current_dir = os.getcwd()\n",
    "print('Current directory: ', current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/oksanaerm/gesture_recognition/data/\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(current_dir, 'data/')\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "val_dir = os.path.join(data_dir, 'val/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "print('Data directory: ', data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kaggle.json\", \"r\") as f:\n",
    "    import json\n",
    "    api_token = json.load(f)\n",
    "os.environ['KAGGLE_USERNAME'] = api_token[\"username\"]\n",
    "os.environ['KAGGLE_KEY'] = api_token[\"key\"]\n",
    "print('Kaggle API token loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d imsparsh/gesture-recognition -p data\n",
    "\n",
    "# Check for the presence of the dataset file\n",
    "if any(file.endswith('.zip') for file in data_dir):\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Dataset not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(os.path.join(data_dir, 'gesture-recognition.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "    print(\"Dataset extracted successfully.\")\n",
    "os.remove(os.path.join(data_dir, 'gesture-recognition.zip'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(image_size: tuple, to_pil: bool = True) -> transforms.Compose:\n",
    "    transform_list = []\n",
    "    if to_pil:\n",
    "        transform_list.append(transforms.ToPILImage())\n",
    "    transform_list.extend([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.gesture_folders = sorted(\n",
    "            [folder for folder in os.listdir(self.root_dir)])\n",
    "        self.class_to_label = {\n",
    "            'Thumbs_Down': 0, \n",
    "            'Right_Swipe': 1, \n",
    "            'Thumbs_Up': 2, \n",
    "            'Left_Swipe': 3, \n",
    "            'Stop': 4}\n",
    "        self.validate_folders_and_labels()\n",
    "    \n",
    "    def validate_folders_and_labels(self):\n",
    "        # Loop through each folder in the root directory and check if it matches any label\n",
    "        unmatched_folders = []\n",
    "        for folder in self.gesture_folders:\n",
    "            normalized_folder_name = folder.replace('_', ' ').lower()\n",
    "            match_found = False\n",
    "            for gesture_type in self.class_to_label.keys():\n",
    "                normalized_gesture_type = gesture_type.replace('_', ' ').lower()\n",
    "                if normalized_gesture_type in normalized_folder_name:\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                unmatched_folders.append(folder)\n",
    "\n",
    "        # If there are unmatched folders, raise a warning or error\n",
    "        if unmatched_folders:\n",
    "            warning_msg = f\"Warning: Unmatched folders found that don't correspond to any label: {unmatched_folders}\"\n",
    "            print(warning_msg)\n",
    "\n",
    "        # Check if there are unused labels in the dictionary\n",
    "        unused_labels = set(self.class_to_label.keys())\n",
    "        for folder in self.gesture_folders:\n",
    "            normalized_folder_name = folder.replace('_', ' ').lower()\n",
    "            for gesture_type in self.class_to_label.keys():\n",
    "                normalized_gesture_type = gesture_type.replace('_', ' ').lower()\n",
    "                if normalized_gesture_type in normalized_folder_name:\n",
    "                    unused_labels.discard(gesture_type)\n",
    "\n",
    "        if unused_labels:\n",
    "            warning_msg = f\"Warning: Unused labels found that don't correspond to any folder: {list(unused_labels)}\"\n",
    "            print(warning_msg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gesture_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gesture_path = os.path.join(self.root_dir, self.gesture_folders[idx])\n",
    "        # Normalize folder name by replacing underscores and converting to lower case\n",
    "        normalized_folder_name = self.gesture_folders[idx].replace(\n",
    "            '_', ' ').lower()\n",
    "        # Try to match normalized folder name with normalized keys from the dictionary\n",
    "        for gesture_type in self.class_to_label.keys():\n",
    "            normalized_gesture_type = gesture_type.replace('_', ' ').lower()\n",
    "            if normalized_gesture_type in normalized_folder_name:\n",
    "                gesture_class_str = gesture_type\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown gesture type in folder name: {self.gesture_folders[idx]}\"\n",
    "                f\"Recognized types are: {', '.join(self.class_to_label.keys())}\")\n",
    "\n",
    "        # Use the dictionary to get the integer label\n",
    "        gesture_class = self.class_to_label.get(gesture_class_str, -1)\n",
    "\n",
    "        frames = []\n",
    "        for img_name in sorted(os.listdir(gesture_path)):\n",
    "            img_path = os.path.join(gesture_path, img_name)\n",
    "            image = Image.open(img_path).convert('RGB') \n",
    "            image = np.array(image) \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            frames.append(image)\n",
    "\n",
    "        frames_tensor = torch.stack(frames, dim=0)\n",
    "        return frames_tensor, gesture_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = (120, 160)\n",
    "SHUFFLE = True\n",
    "\n",
    "# Initialize transforms\n",
    "transform_data_prep = get_transform((120, 160), to_pil=True)\n",
    "\n",
    "# Initialize train dataset and DataLoader\n",
    "train_dataset = GestureDataset(\n",
    "    root_dir=train_dir, transform=transform_data_prep)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "\n",
    "# Initialize validation dataset and DataLoader\n",
    "val_dataset = GestureDataset(root_dir=val_dir, transform=transform_data_prep)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# Initialize weights\n",
    "class SEBlock(nn.Module):  # to recalibrate the feature maps\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "# This could be quite useful for recognizing gestures where some features \n",
    "# (like the shape and movement of the hand) are more critical than others \n",
    "# (like the background).\n",
    "\n",
    "# Define the model\n",
    "class Deep3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, reduction=16, dropout_rate=0.2):\n",
    "        super(Deep3DCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=30, out_channels=64, kernel_size=(1, 3, 3), padding=(0, 1, 1), groups=2),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(64, reduction=reduction),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            \n",
    "            nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(1, 3, 3), padding=(0, 1, 1), groups=4),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(128, reduction=reduction),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Dropout3d(dropout_rate),\n",
    "\n",
    "            nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(1, 3, 3), padding=(0, 1, 1), groups=4),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(256, reduction=reduction),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Dropout3d(dropout_rate),\n",
    "        )\n",
    "        # Adaptive pool to make output size (batch_size, channels, 1, 1, 1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(256, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # Initialize weights using Xavier initialization to improve convergence\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 166/166 [03:39<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2697 Acc: 42.8356\n",
      "Train F1: 0.4218, Precision: 0.4348, Recall: 0.4284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 14: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0453 Acc: 60.0000\n",
      "Val F1: 0.5679, Precision: 0.5993, Recall: 0.6000\n",
      "New best validation loss: 1.045254635810852 at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 166/166 [03:31<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3032 Acc: 41.6290\n",
      "Train F1: 0.4176, Precision: 0.4202, Recall: 0.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 15: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.1345 Acc: 48.0000\n",
      "Val F1: 0.4376, Precision: 0.5076, Recall: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2583 Acc: 43.7406\n",
      "Train F1: 0.4310, Precision: 0.4278, Recall: 0.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 16: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0623 Acc: 50.0000\n",
      "Val F1: 0.4449, Precision: 0.5204, Recall: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2602 Acc: 42.6848\n",
      "Train F1: 0.4220, Precision: 0.4231, Recall: 0.4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 17: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0534 Acc: 62.0000\n",
      "Val F1: 0.5988, Precision: 0.6268, Recall: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 166/166 [03:31<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2460 Acc: 45.7014\n",
      "Train F1: 0.4578, Precision: 0.4612, Recall: 0.4570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 18: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0502 Acc: 55.0000\n",
      "Val F1: 0.5151, Precision: 0.6149, Recall: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 166/166 [03:34<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2756 Acc: 42.6848\n",
      "Train F1: 0.4222, Precision: 0.4206, Recall: 0.4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 19: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0418 Acc: 54.0000\n",
      "Val F1: 0.4994, Precision: 0.6237, Recall: 0.5400\n",
      "New best validation loss: 1.0418296957015991 at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 166/166 [03:33<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2635 Acc: 45.3997\n",
      "Train F1: 0.4500, Precision: 0.4477, Recall: 0.4540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 20: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0141 Acc: 64.0000\n",
      "Val F1: 0.6182, Precision: 0.6564, Recall: 0.6400\n",
      "New best validation loss: 1.0140574407577514 at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 166/166 [03:31<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2366 Acc: 43.8914\n",
      "Train F1: 0.4361, Precision: 0.4365, Recall: 0.4389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 21: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0370 Acc: 63.0000\n",
      "Val F1: 0.6059, Precision: 0.6577, Recall: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 166/166 [03:30<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2688 Acc: 42.2323\n",
      "Train F1: 0.4151, Precision: 0.4224, Recall: 0.4223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 22: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9972 Acc: 66.0000\n",
      "Val F1: 0.6382, Precision: 0.6791, Recall: 0.6600\n",
      "New best validation loss: 0.9972357785701752 at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2277 Acc: 45.8522\n",
      "Train F1: 0.4532, Precision: 0.4515, Recall: 0.4585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 23: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 1.0033 Acc: 66.0000\n",
      "Val F1: 0.6236, Precision: 0.6631, Recall: 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 166/166 [03:30<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2393 Acc: 46.7572\n",
      "Train F1: 0.4650, Precision: 0.4639, Recall: 0.4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 24: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9905 Acc: 64.0000\n",
      "Val F1: 0.6150, Precision: 0.6837, Recall: 0.6400\n",
      "New best validation loss: 0.9904718971252442 at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1923 Acc: 50.8296\n",
      "Train F1: 0.5062, Precision: 0.5061, Recall: 0.5083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 25: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9867 Acc: 66.0000\n",
      "Val F1: 0.6316, Precision: 0.6687, Recall: 0.6600\n",
      "New best validation loss: 0.9866794180870057 at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 166/166 [03:34<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3098 Acc: 45.8522\n",
      "Train F1: 0.4554, Precision: 0.4568, Recall: 0.4585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 26: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9683 Acc: 64.0000\n",
      "Val F1: 0.6304, Precision: 0.6680, Recall: 0.6400\n",
      "New best validation loss: 0.968312736749649 at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 166/166 [03:32<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2048 Acc: 48.4163\n",
      "Train F1: 0.4816, Precision: 0.4832, Recall: 0.4842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 27: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9791 Acc: 66.0000\n",
      "Val F1: 0.6241, Precision: 0.6653, Recall: 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 166/166 [03:37<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1941 Acc: 48.8688\n",
      "Train F1: 0.4887, Precision: 0.4897, Recall: 0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 28: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9915 Acc: 62.0000\n",
      "Val F1: 0.6095, Precision: 0.6525, Recall: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2370 Acc: 47.0588\n",
      "Train F1: 0.4698, Precision: 0.4697, Recall: 0.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 29: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9991 Acc: 59.0000\n",
      "Val F1: 0.5724, Precision: 0.6351, Recall: 0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 166/166 [03:30<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2456 Acc: 43.7406\n",
      "Train F1: 0.4373, Precision: 0.4389, Recall: 0.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 30: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9816 Acc: 64.0000\n",
      "Val F1: 0.6164, Precision: 0.6462, Recall: 0.6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 166/166 [03:31<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2185 Acc: 47.0588\n",
      "Train F1: 0.4695, Precision: 0.4695, Recall: 0.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 31: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9392 Acc: 66.0000\n",
      "Val F1: 0.6418, Precision: 0.6698, Recall: 0.6600\n",
      "New best validation loss: 0.9391726022958755 at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 166/166 [03:31<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2232 Acc: 46.7572\n",
      "Train F1: 0.4617, Precision: 0.4647, Recall: 0.4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 32: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9189 Acc: 67.0000\n",
      "Val F1: 0.6581, Precision: 0.6902, Recall: 0.6700\n",
      "New best validation loss: 0.9188961338996887 at epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 166/166 [03:32<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2020 Acc: 50.2262\n",
      "Train F1: 0.5006, Precision: 0.4995, Recall: 0.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 33: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9326 Acc: 64.0000\n",
      "Val F1: 0.6059, Precision: 0.6753, Recall: 0.6400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 166/166 [03:32<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1977 Acc: 47.2097\n",
      "Train F1: 0.4687, Precision: 0.4673, Recall: 0.4721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 34: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9505 Acc: 65.0000\n",
      "Val F1: 0.6293, Precision: 0.6888, Recall: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 166/166 [03:34<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2197 Acc: 47.5113\n",
      "Train F1: 0.4715, Precision: 0.4703, Recall: 0.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 35: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9575 Acc: 57.0000\n",
      "Val F1: 0.5585, Precision: 0.6052, Recall: 0.5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 166/166 [03:30<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1891 Acc: 51.1312\n",
      "Train F1: 0.5087, Precision: 0.5091, Recall: 0.5113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 36: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9970 Acc: 58.0000\n",
      "Val F1: 0.5624, Precision: 0.5958, Recall: 0.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 166/166 [03:32<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1728 Acc: 48.5671\n",
      "Train F1: 0.4825, Precision: 0.4835, Recall: 0.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 37: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8738 Acc: 68.0000\n",
      "Val F1: 0.6635, Precision: 0.6815, Recall: 0.6800\n",
      "New best validation loss: 0.8737629687786103 at epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 166/166 [03:30<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1974 Acc: 49.7738\n",
      "Train F1: 0.4945, Precision: 0.4934, Recall: 0.4977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 38: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8892 Acc: 68.0000\n",
      "Val F1: 0.6555, Precision: 0.7173, Recall: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1164 Acc: 55.6561\n",
      "Train F1: 0.5553, Precision: 0.5559, Recall: 0.5566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 39: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9319 Acc: 61.0000\n",
      "Val F1: 0.5964, Precision: 0.6569, Recall: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 166/166 [03:31<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1674 Acc: 50.5279\n",
      "Train F1: 0.5061, Precision: 0.5075, Recall: 0.5053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 40: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.8832 Acc: 65.0000\n",
      "Val F1: 0.6295, Precision: 0.6938, Recall: 0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 166/166 [03:31<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1722 Acc: 48.5671\n",
      "Train F1: 0.4854, Precision: 0.4863, Recall: 0.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 41: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9429 Acc: 62.0000\n",
      "Val F1: 0.5961, Precision: 0.6605, Recall: 0.6200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 166/166 [03:35<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1856 Acc: 48.5671\n",
      "Train F1: 0.4835, Precision: 0.4852, Recall: 0.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 42: 100%|██████████| 25/25 [00:19<00:00,  1.30it/s]\n",
      "[I 2023-09-28 03:21:02,120] Trial 4 finished with value: 0.9140139985084533 and parameters: {'lr': 1.878887835176021e-05, 'weight_decay': 1.0377220058734007e-10, 'dropout_rate': 0.29168643854284715, 'reduction': 8}. Best is trial 1 with value: 0.606099152341485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.9140 Acc: 60.0000\n",
      "Val F1: 0.5848, Precision: 0.6076, Recall: 0.6000\n",
      "Early stopping after 5 epochs with no improvement.\n",
      "Number of finished trials:  5\n",
      "Best trial:\n",
      "Value:  0.606099152341485\n",
      "Params: \n",
      "    lr: 3.150586493930411e-05\n",
      "    weight_decay: 4.8570077543617684e-08\n",
      "    dropout_rate: 0.10545447017727612\n",
      "    reduction: 24\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Function to save the checkpoint of the model\n",
    "def save_checkpoint(model, optimizer, epoch, filepath='best_gesture_recog.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filepath)\n",
    "\n",
    "# Function to load a checkpoint and restore model and optimizer states\n",
    "def load_checkpoint(model, optimizer, filepath='best_gesture_recog.pth'):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "\n",
    "def main_training_loop(epoch, model, train_dataloader, criterion, optimizer, writer, use_cuda, scaler):\n",
    "    model.train()\n",
    "    # Initialize metrics\n",
    "    epoch_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    y_true_train = []\n",
    "    y_pred_train = []\n",
    "    # Iterate through each batch from the training data\n",
    "    for i, (inputs, labels) in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")):\n",
    "        if use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with Automatic Mixed Precision training\n",
    "        with autocast(enabled=use_cuda):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            # Clip gradients to avoid \"exploding gradient\" problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Accumulate metrics for this epoch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Save the true and predicted labels for further metrics calculation\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_pred_train.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    # Calculate and log metrics for this epoch\n",
    "    epoch_loss /= len(train_dataloader.dataset)\n",
    "    train_accuracy = 100 * correct_train / total_train \n",
    "    f1 = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "    precision = precision_score(y_true_train, y_pred_train, average='weighted')\n",
    "    recall = recall_score(y_true_train, y_pred_train, average='weighted')\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('F1/train', f1, epoch)\n",
    "    writer.add_scalar('Precision/train', precision, epoch)\n",
    "    writer.add_scalar('Recall/train', recall, epoch)\n",
    "\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, train_accuracy))\n",
    "    print(\n",
    "        f'Train F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "    return epoch_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validation_loop(epoch, model, val_dataloader, criterion, writer, use_cuda):\n",
    "    model.eval()\n",
    "    # Initialize metrics\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0 \n",
    "    total_val = 0\n",
    "    y_true_val = []\n",
    "    y_pred_val = []\n",
    "\n",
    "    # Iterate through each batch from the validation data\n",
    "    # We don't need to calculate gradients here, so we use torch.no_grad() \n",
    "    # context manager to save memory\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels, ) in enumerate(tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}\")):\n",
    "            if use_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Accumulate metrics for this epoch\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save the true and predicted labels for further metrics calculation\n",
    "            y_true_val.extend(labels.cpu().numpy())\n",
    "            y_pred_val.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for this epoch\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    f1 = f1_score(y_true_val, y_pred_val, average='weighted')\n",
    "    precision = precision_score(y_true_val, y_pred_val, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_true_val, y_pred_val, average='weighted')\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/val', f1, epoch)\n",
    "    writer.add_scalar('Precision/val', precision, epoch)\n",
    "    writer.add_scalar('Recall/val', recall, epoch)\n",
    "    print('Val Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_accuracy))\n",
    "    print(\n",
    "        f'Val F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def objective(trial, train_dataloader, val_dataloader):\n",
    "    config = {\n",
    "        'num_classes': 5,\n",
    "        'num_epochs': 50,\n",
    "        'log_interval': 10,\n",
    "        'batch_size': 32,\n",
    "        'early_stopping_patience': 5,\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-10, 1e-3, log=True),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        'reduction': trial.suggest_int('reduction', 8, 32, step=4)\n",
    "    }\n",
    "\n",
    "    model = Deep3DCNN(num_classes=config['num_classes'],\n",
    "                      reduction=config['reduction'], dropout_rate=config['dropout_rate'])\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=config['early_stopping_patience'], verbose=True)\n",
    "\n",
    "    # Check CUDA availability and initialize GradScaler\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_cuda else None\n",
    "\n",
    "    # Inform the user only once about CUDA status\n",
    "    if use_cuda:\n",
    "        print(\"CUDA is available. Running on GPU.\")\n",
    "    else:\n",
    "        print(\"Warning: CUDA not available. Running on CPU.\")\n",
    "\n",
    "    # Early Stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # TensorBoard Writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training and Validation Loop\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        epoch_loss, train_accuracy = main_training_loop(\n",
    "            epoch, model, train_dataloader, criterion, optimizer, writer, use_cuda, scaler)\n",
    "        val_loss, val_accuracy = validation_loop(\n",
    "            epoch, model, val_dataloader, criterion, writer, use_cuda)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping and model saving logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            writer.add_scalar('Best/val_loss', best_val_loss, best_epoch)\n",
    "            print(\n",
    "                f\"New best validation loss: {best_val_loss} at epoch {best_epoch+1}\")\n",
    "            save_checkpoint(model, optimizer, epoch)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= config['early_stopping_patience']:\n",
    "            print(\n",
    "                f\"Early stopping after {config['early_stopping_patience']} epochs with no improvement.\")\n",
    "            # Load the best checkpoint\n",
    "            load_checkpoint(model, optimizer)\n",
    "            break\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Optuna study to find the best hyperparameters\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_loader,\n",
    "                   val_loader), n_trials=5)  # Number of trials\n",
    "\n",
    "    # Results\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('Value: ', trial.value)\n",
    "    print('Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation for real-time prediction\n",
    "transform_real_time = get_transform((120, 160), to_pil=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the camera\n",
    "camera = cv2.VideoCapture(1)\n",
    "if not camera.isOpened():\n",
    "    print(\"Error: Camera could not be opened.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Load the entire checkpoint dictionary\n",
    "checkpoint = torch.load('best_gesture_recog.pth')\n",
    "\n",
    "model = Deep3DCNN(num_classes=5, reduction=8)\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Initialize deque for storing recent frames\n",
    "recent_frames = deque(maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 3\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 2\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n",
      "Predicted Gesture: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/z7/_d_mxw0j3r749c4wqtx9ny_m0000gn/T/ipykernel_20666/1746622877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to grab frame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "    # Convert BGR to RGB and transform\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb_frame = Image.fromarray(rgb_frame)\n",
    "    transformed_frame = transform_real_time(rgb_frame)\n",
    "\n",
    "    # Add the transformed frame to deque and continue with prediction-related operations\n",
    "    recent_frames.append(transformed_frame)\n",
    "\n",
    "    if len(recent_frames) == 30:\n",
    "        frame_count += 1\n",
    "        if frame_count % 10 == 0: \n",
    "            # Make a prediction every 10 frames\n",
    "            test_sample = torch.stack(list(recent_frames), dim=0).unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                test_sample = test_sample.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(test_sample)\n",
    "                _, prediction = torch.max(output.data, 1)\n",
    "\n",
    "            print(f\"Predicted Gesture: {prediction.item()}\")\n",
    "\n",
    "            # Display prediction on the frame\n",
    "            cv2.putText(frame, f\"Gesture: {prediction.item()}\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a27c97cdfd93c0dbfbc1217e390474b7a3939b486f21c5a485c92fc29bd040d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
