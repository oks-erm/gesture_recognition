{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import zipfile\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "os.chdir(os.path.dirname(current_dir))\n",
    "current_dir = os.getcwd()\n",
    "print('Current directory: ', current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(current_dir, 'data/')\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "val_dir = os.path.join(data_dir, 'val/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "print('Data directory: ', data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kaggle.json\", \"r\") as f:\n",
    "    import json\n",
    "    api_token = json.load(f)\n",
    "os.environ['KAGGLE_USERNAME'] = api_token[\"username\"]\n",
    "os.environ['KAGGLE_KEY'] = api_token[\"key\"]\n",
    "print('Kaggle API token loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d imsparsh/gesture-recognition -p data\n",
    "\n",
    "# Check for the presence of the dataset file\n",
    "if any(file.endswith('.zip') for file in data_dir):\n",
    "    print(\"Dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Dataset not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(os.path.join(data_dir, 'gesture-recognition.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "    print(\"Dataset extracted successfully.\")\n",
    "os.remove(os.path.join(data_dir, 'gesture-recognition.zip'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(image_size: tuple) -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.gesture_folders = sorted(\n",
    "            [folder for folder in os.listdir(self.root_dir)])\n",
    "        self.class_to_label = {\n",
    "            'Thumbs_Down': 0, \n",
    "            'Right_Swipe': 1, \n",
    "            'Thumbs_Up': 2, \n",
    "            'Left_Swipe': 3, \n",
    "            'Stop': 4}\n",
    "        self.validate_folders_and_labels()\n",
    "    \n",
    "    def validate_folders_and_labels(self):\n",
    "        # Loop through each folder in the root directory and check if it matches any label\n",
    "        unmatched_folders = []\n",
    "        for folder in self.gesture_folders:\n",
    "            normalized_folder_name = folder.replace('_', ' ').lower()\n",
    "            match_found = False\n",
    "            for gesture_type in self.class_to_label.keys():\n",
    "                normalized_gesture_type = gesture_type.replace('_', ' ').lower()\n",
    "                if normalized_gesture_type in normalized_folder_name:\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                unmatched_folders.append(folder)\n",
    "\n",
    "        # If there are unmatched folders, raise a warning or error\n",
    "        if unmatched_folders:\n",
    "            warning_msg = f\"Warning: Unmatched folders found that don't correspond to any label: {unmatched_folders}\"\n",
    "            print(warning_msg)\n",
    "\n",
    "        # Check if there are unused labels in the dictionary\n",
    "        unused_labels = set(self.class_to_label.keys())\n",
    "        for folder in self.gesture_folders:\n",
    "            normalized_folder_name = folder.replace('_', ' ').lower()\n",
    "            for gesture_type in self.class_to_label.keys():\n",
    "                normalized_gesture_type = gesture_type.replace('_', ' ').lower()\n",
    "                if normalized_gesture_type in normalized_folder_name:\n",
    "                    unused_labels.discard(gesture_type)\n",
    "\n",
    "        if unused_labels:\n",
    "            warning_msg = f\"Warning: Unused labels found that don't correspond to any folder: {list(unused_labels)}\"\n",
    "            print(warning_msg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gesture_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gesture_path = os.path.join(self.root_dir, self.gesture_folders[idx])\n",
    "        # Normalize folder name by replacing underscores and converting to lower case\n",
    "        normalized_folder_name = self.gesture_folders[idx].replace(\n",
    "            '_', ' ').lower()\n",
    "        # Try to match normalized folder name with normalized keys from the dictionary\n",
    "        for gesture_type in self.class_to_label.keys():\n",
    "            normalized_gesture_type = gesture_type.replace('_', ' ').lower()\n",
    "            if normalized_gesture_type in normalized_folder_name:\n",
    "                gesture_class_str = gesture_type\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown gesture type in folder name: {self.gesture_folders[idx]}\"\n",
    "                f\"Recognized types are: {', '.join(self.class_to_label.keys())}\")\n",
    "\n",
    "        # Use the dictionary to get the integer label\n",
    "        gesture_class = self.class_to_label.get(gesture_class_str, -1)\n",
    "\n",
    "        frames = []\n",
    "        for img_name in sorted(os.listdir(gesture_path)):\n",
    "            img_path = os.path.join(gesture_path, img_name)\n",
    "            image = Image.open(img_path).convert('RGB') \n",
    "            image = np.array(image) \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            frames.append(image)\n",
    "\n",
    "        frames_tensor = torch.stack(frames, dim=0)\n",
    "        return frames_tensor, gesture_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "IMAGE_SIZE = (120, 160)\n",
    "SHUFFLE = True\n",
    "\n",
    "# Initialize transforms\n",
    "transform = get_transform(IMAGE_SIZE)\n",
    "\n",
    "# Initialize train dataset and DataLoader\n",
    "train_dataset = GestureDataset(\n",
    "    root_dir=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "\n",
    "# Initialize validation dataset and DataLoader\n",
    "val_dataset = GestureDataset(root_dir=val_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "# Initialize weights\n",
    "class SEBlock(nn.Module):  # to recalibrate the feature maps\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "# This could be quite useful for recognizing gestures where some features \n",
    "# (like the shape and movement of the hand) are more critical than others \n",
    "# (like the background).\n",
    "\n",
    "# Define the model\n",
    "class Deep3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, reduction=16, dropout_rate=0.2):\n",
    "        super(Deep3DCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=30, out_channels=64, kernel_size=(1, 3, 3), padding=(0, 1, 1), groups=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(64, reduction=reduction),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            \n",
    "            nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(1, 3, 3), padding=(0, 1, 1), groups=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(128, reduction=reduction),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Dropout3d(dropout_rate),\n",
    "\n",
    "            nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(1, 3, 3), padding=(0, 1, 1), groups=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(256, reduction=reduction),\n",
    "            nn.MaxPool3d((1, 2, 2), stride=(1, 2, 2)),\n",
    "            nn.Dropout3d(dropout_rate),\n",
    "        )\n",
    "        # Adaptive pool to make output size (batch_size, channels, 1, 1, 1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(256, 2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # Initialize weights using Xavier initialization to improve convergence\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                init.xavier_normal_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-20 17:52:10,883] A new study created in memory with name: no-name-548b9b6a-0cb3-424e-8f1e-70b79f785b7c\n",
      "Training Epoch 1:  31%|███       | 51/166 [02:13<05:04,  2.65s/it]"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Function to save the checkpoint of the model\n",
    "def save_checkpoint(model, optimizer, epoch, filepath='best_gesture_recog.pth'):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, filepath)\n",
    "\n",
    "# Function to load a checkpoint and restore model and optimizer states\n",
    "def load_checkpoint(model, optimizer, filepath='best_gesture_recog.pth'):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "\n",
    "def main_training_loop(epoch, model, train_dataloader, criterion, optimizer, writer, use_cuda, scaler):\n",
    "    model.train()\n",
    "    # Initialize metrics\n",
    "    epoch_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    y_true_train = []\n",
    "    y_pred_train = []\n",
    "    # Iterate through each batch from the training data\n",
    "    for i, (inputs, labels) in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\")):\n",
    "        if use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with Automatic Mixed Precision training\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            # Clip gradients to avoid \"exploding gradient\" problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Accumulate metrics for this epoch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Save the true and predicted labels for further metrics calculation\n",
    "        y_true_train.extend(labels.cpu().numpy())\n",
    "        y_pred_train.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    # Calculate and log metrics for this epoch\n",
    "    epoch_loss /= len(train_dataloader.dataset)\n",
    "    train_accuracy = 100 * correct_train / total_train \n",
    "    f1 = f1_score(y_true_train, y_pred_train, average='weighted')\n",
    "    precision = precision_score(y_true_train, y_pred_train, average='weighted')\n",
    "    recall = recall_score(y_true_train, y_pred_train, average='weighted')\n",
    "    \n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('F1/train', f1, epoch)\n",
    "    writer.add_scalar('Precision/train', precision, epoch)\n",
    "    writer.add_scalar('Recall/train', recall, epoch)\n",
    "\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, train_accuracy))\n",
    "    print(\n",
    "        f'Train F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "    return epoch_loss, train_accuracy\n",
    "\n",
    "\n",
    "def validation_loop(epoch, model, val_dataloader, criterion, writer, use_cuda):\n",
    "    model.eval()\n",
    "    # Initialize metrics\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0 \n",
    "    total_val = 0\n",
    "    y_true_val = []\n",
    "    y_pred_val = []\n",
    "\n",
    "    # Iterate through each batch from the validation data\n",
    "    # We don't need to calculate gradients here, so we use torch.no_grad() \n",
    "    # context manager to save memory\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels, ) in enumerate(tqdm(val_dataloader, desc=f\"Validation Epoch {epoch + 1}\")):\n",
    "            if use_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Accumulate metrics for this epoch\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            # Save the true and predicted labels for further metrics calculation\n",
    "            y_true_val.extend(labels.cpu().numpy())\n",
    "            y_pred_val.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for this epoch\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    f1 = f1_score(y_true_val, y_pred_val, average='weighted')\n",
    "    precision = precision_score(y_true_val, y_pred_val, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_true_val, y_pred_val, average='weighted')\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "    writer.add_scalar('F1/val', f1, epoch)\n",
    "    writer.add_scalar('Precision/val', precision, epoch)\n",
    "    writer.add_scalar('Recall/val', recall, epoch)\n",
    "    print('Val Loss: {:.4f} Acc: {:.4f}'.format(val_loss, val_accuracy))\n",
    "    print(\n",
    "        f'Val F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def objective(trial, train_dataloader, val_dataloader):\n",
    "    config = {\n",
    "        'num_classes': 5,\n",
    "        'num_epochs': 50,\n",
    "        'log_interval': 10,\n",
    "        'batch_size': 32,\n",
    "        'early_stopping_patience': 5,\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-10, 1e-3, log=True),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        'reduction': trial.suggest_int('reduction', 8, 32, step=4)\n",
    "    }\n",
    "\n",
    "    model = Deep3DCNN(num_classes=config['num_classes'],\n",
    "                      reduction=config['reduction'], dropout_rate=config['dropout_rate'])\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=config['early_stopping_patience'], verbose=True)\n",
    "\n",
    "    # Check CUDA availability and initialize GradScaler\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_cuda else None\n",
    "\n",
    "    # Early Stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # TensorBoard Writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training and Validation Loop\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        epoch_loss, train_accuracy = main_training_loop(\n",
    "            epoch, model, train_dataloader, criterion, optimizer, writer, use_cuda, scaler)\n",
    "        val_loss, val_accuracy = validation_loop(\n",
    "            epoch, model, val_dataloader, criterion, writer, use_cuda)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping and model saving logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            writer.add_scalar('Best/val_loss', best_val_loss, best_epoch)\n",
    "            print(\n",
    "                f\"New best validation loss: {best_val_loss} at epoch {best_epoch}\")\n",
    "            save_checkpoint(model, optimizer, epoch)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= config['early_stopping_patience']:\n",
    "            print(\n",
    "                f\"Early stopping after {config['early_stopping_patience']} epochs with no improvement.\")\n",
    "            # Load the best checkpoint\n",
    "            load_checkpoint(model, optimizer)\n",
    "            break\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Optuna study to find the best hyperparameters\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, train_loader,\n",
    "                   val_loader), n_trials=50)  # Number of trials\n",
    "\n",
    "    # Results\n",
    "    print('Number of finished trials: ', len(study.trials))\n",
    "    print('Best trial:')\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print('Value: ', trial.value)\n",
    "    print('Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((120, 160)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Initialize the camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the model and set it to evaluation mode\n",
    "model = Deep3DCNN(num_classes=5)\n",
    "model.load_state_dict(torch.load('best_gesture_recog.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Initialize deque for storing recent frames\n",
    "recent_frames = deque(maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb_frame = Image.fromarray(rgb_frame)\n",
    "    transformed_frame = transform(rgb_frame)\n",
    "\n",
    "    # Add the frame to deque\n",
    "    recent_frames.append(transformed_frame)\n",
    "\n",
    "    if len(recent_frames) == 30:\n",
    "        # Make a prediction\n",
    "        test_sample = torch.stack(list(recent_frames), dim=0).unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            test_sample = test_sample.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(test_sample)\n",
    "            _, prediction = torch.max(output.data, 1)\n",
    "\n",
    "        print(f\"Predicted Gesture: {prediction.item()}\")\n",
    "\n",
    "        # Display prediction on the frame\n",
    "        cv2.putText(frame, f\"Gesture: {prediction.item()}\", (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a27c97cdfd93c0dbfbc1217e390474b7a3939b486f21c5a485c92fc29bd040d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
